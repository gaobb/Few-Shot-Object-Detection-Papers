| Method | Venue | Year| Backbone|Detector|Paradigm|Setting| Set1 1/2/3/5/10-shot |  Set2 1/2/3/5/10-shot | Set3 1/2/3/5/10-shot |Code|
| :-----|:-----:|:-----:|:---:|:---:|:----:|:-----|:-----:|:-----:|:----:|:-----|
[DCFS](https://openreview.net/pdf?id=dVXO3Orjmxk)| NeurIPS | 2022| R-101| Faster-RCNN |Fine-tuning|FSOD| 56.6 59.6 62.9 65.6 62.5| 29.7 38.7 46.2 48.9 48.1| 47.9 51.9 53.3 56.1 59.4|[PyTorch](https://csgaobb.github.io/Projects/DCFS)|
[DCFS](https://openreview.net/pdf?id=dVXO3Orjmxk)| NeurIPS | 2022| R-101| Faster-RCNN |Fine-tuning|gFSOD|45.8 59.1 62.1 66.8 68.0| 31.8 41.7 46.6 50.3 53.7| 39.6 52.1 56.3 60.3 63.3|[PyTorch](https://csgaobb.github.io/Projects/DCFS)|
|[CoCo-RCNN](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136860056.pdf)| ECCV |2022|R-101|Sparse-RCNN| Fine-tuning |FSOD|33.5 44.2 50.2 57.5 63.3 | 25.3 31.0 39.6 43.8 50.1 | 24.8 36.9 42.8 50.8 57.7|[PyTorch](https://github.com/Phoenix-V/coco-rcnn)|
[FewX](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136790707.pdf)|ECCV|2022| R-50 |Faster R-CNN|Fine-tuning|FSOD|40.1 44.2 51.2 62.0 63.0| 33.3 33.1 42.3 46.3 52.3| 36.1 43.1 43.5 52.0 56.0|[PyTorch](https://github.com/fanq15/FewX)|
[MFDC](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136690569.pdf)|ECCV|2022| R-101 |Faster R-CNN|Fine-tuning|FSOD|63.4 66.3 67.7 69.4 68.1| 42.1 46.5 53.4 55.3 53.8 | 56.1 58.3 59.0 62.2 63.7|[PyTorch](https://github.com/WuShuang1998/MFDC)|
[TENET](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136800300.pdf)|ECCV|2022|R-50|-|Fine-tuning|FSOD|46.7 - 55.4 62.3 66.9| 40.3 - 44.7 49.3 52.1| 35.5 - 46.0 54.4 54.6 |[PyTorch](https://github.com/ZS123-lang/TENET)|
|[MRSN](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136800388.pdf)|ECCV|2022|R-101|Faster R-CNN|Fine-tuning|FSOD|47.6 48.6 57.8 61.9 62.6| 31.2 38.3 46.7 47.1 50.6| 35.5 30.9 45.6 54.4 57.4|[-](https://github.com/MMatx/MRSN)|
|[KD-DeFRCN](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700279.pdf)|ECCV|2022|R-101|Faster R-CNN|Fine-tuning|FSOD|58.2 62.5 65.1  68.2 67.4| 37.6 45.6 52.0 54.6 53.2| 53.8 57.7 58.0 62.4 62.2 |-|
|[Label, Verify, Correct](https://openaccess.thecvf.com/content/CVPR2022/papers/Kaul_Label_Verify_Correct_A_Simple_Few_Shot_Object_Detection_Method_CVPR_2022_paper.pdf)| CVPR |2022|R-101+DINO ViT-S|Faster R-CNN|Fine-tuning|FSOD| 54.5 53.2 58.8 63.2 65.7| 32.8 29.2 50.7 49.8 50.6| 48.4 52.7 55.0 59.6 59.6|[PyTorch](https://github.com/prannaykaul/lvc)|
|[FCT](https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Few-Shot_Object_Detection_With_Fully_Cross-Transformer_CVPR_2022_paper.pdf)| CVPR |2022|PVTv2-B2-Li| Faster R-CNN |meta-learning|FSOD|38.5 49.6 53.5 59.8 64.3| 25.9 34.2 40.1 44.9 47.4| 34.7 43.9 49.3 53.1 56.3|[PyTorch](https://github.com/GuangxingHan/FCT)|
|[KFSOD](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Kernelized_Few-Shot_Object_Detection_With_Efficient_Integral_Aggregation_CVPR_2022_paper.pdf)| CVPR |2022|R-50|Faster-RCN|meta-learning|FSOD|44.6 - 54.4 60.9 65.8 | 37.8 - 43.1 48.1 50.4 | 34.8 - 44.1 52.7 53.9|[-](https://github.com/ZS123-lang/KFSOD)|
|[Meta-Faster-RCNN](https://arxiv.org/abs/2104.07719)| AAAI |2022|R-101|Faster R-CNN| meta-learning|FSOD|43.0 54.5 60.6 66.1 65.4 |27.7 35.5 46.1 47.8 51.4 |40.6 46.4 53.4 59.9 58.6|[PyTorch](https://github.com/GuangxingHan/Meta-Faster-R-CNN)|
|[QA-FewDet](https://arxiv.org/abs/2112.09791)| ICCV |2021|R-101|Faster R-CNN| meta-learning|FSOD|42.4 51.9 55.7 62.6 63.4| 25.9 37.8 46.6 48.9 51.1| 35.2 42.9 47.8 54.8 53.5|[PyTorch](https://github.com/GuangxingHan/QA-FewDet)|
|[DeFRCN](https://openaccess.thecvf.com/content/ICCV2021/papers/Qiao_DeFRCN_Decoupled_Faster_R-CNN_for_Few-Shot_Object_Detection_ICCV_2021_paper.pdf) | ICCV | 2021|R-101|Faster R-CNN| Fine-tuning|FSOD|53.6 57.5 61.5 64.1 60.8 | 30.1 38.1 47.0 53.3 47.9 | 48.4 50.9 52.3 54.9 57.4|[PyTorch](https://github.com/er-muyue/DeFRCN)
|[DeFRCN](https://openaccess.thecvf.com/content/ICCV2021/papers/Qiao_DeFRCN_Decoupled_Faster_R-CNN_for_Few-Shot_Object_Detection_ICCV_2021_paper.pdf) | ICCV | 2021|R-101|Faster R-CNN| Fine-tuning|gFSOD|40.2 53.6 58.2  63.6 66.5 | 29.5 39.7 43.4 48.1 52.8 | 35.0 38.3 52.9 57.7 60.8|[PyTorch](https://github.com/er-muyue/DeFRCN)
|[FSOD$^{up}$](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Universal-Prototype_Enhancing_for_Few-Shot_Object_Detection_ICCV_2021_paper.pdf)| ICCV |2021|R-101|Faster R-CNN|Fine-tuning|gFSOD|43.8 47.8 50.3 55.4 61.7 | 31.2 30.5 41.2 42.2 48.3| 35.5 39.7 43.9 50.6 53.5|[PyTorch](https://github.com/AmingWu/UP-FSOD)
|[FADI](https://proceedings.neurips.cc/paper/2021/file/8a1e808b55fde9455cb3d8857ed88389-Paper.pdf)| NeurIPS |2021|R-101|Faster R-CNN|Fine-tuning|gFSOD| 50.3 54.8 54.2 59.3 63.2|30.6 35.0 40.3 42.8 48.0 |45.7 49.7 49.1 55.0 59.6|[PyTorch](https://github.com/yhcao6/FADI)|
|[SRR-FSD](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_Semantic_Relation_Reasoning_for_Shot-Stable_Few-Shot_Object_Detection_CVPR_2021_paper.pdf)|CVPR|2021|R-101|Faster R-CNN|Fine-tuning|FSOD| 47.8 50.5 51.3 55.2 56.8 |32.5 35.3 39.1 40.8 43.8 |40.1 41.5 44.3 46.9 46.4|-|
|[CME](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Beyond_Max-Margin_Class_Margin_Equilibrium_for_Few-Shot_Object_Detection_CVPR_2021_paper.pdf)|CVPR|2021||F-RCNN|meta-learning|gFSOD|41.5 47.5 50.4 58.2 60.9| 27.2 30.2 41.4 42.5 46.8 |34.3 39.6 45.1 48.3 51.5|[PyTorch](https://github.com/Bohao-Lee/CME)|
|[CME](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Beyond_Max-Margin_Class_Margin_Equilibrium_for_Few-Shot_Object_Detection_CVPR_2021_paper.pdf)|CVPR|2021||Meta-YOLO|meta-learning|gFSOD|17.8 26.1 31.5 44.8 47.5| 12.7 17.4 27.1 33.7 40.0| 15.7 27.4 30.7 44.9 48.8|[PyTorch](https://github.com/Bohao-Lee/CME)|
|[DCNet](https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Dense_Relation_Distillation_With_Context-Aware_Aggregation_for_Few-Shot_Object_Detection_CVPR_2021_paper.pdf)|CVPR|2021|R-101|Faster R-CNN|meta-learning|FSOD| 33.9 37.4 43.7 51.1 59.6 |23.2 24.8 30.6 36.7 46.6 |32.3 34.9 39.7 42.6 50.7|[](https://github.com/hzhupku/DCNet)|
|[TIP](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Transformation_Invariant_Few-Shot_Object_Detection_CVPR_2021_paper.pdf)|CVPR|2021|R-101|Faster R-CNN|meta-learning|FSOD|27.7 36.5 43.3 50.2 59.6| 22.7 30.1 33.8 40.9 46.9| 21.7 30.6 38.1 44.5 50.9|
|[FSCE](https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_FSCE_Few-Shot_Object_Detection_via_Contrastive_Proposal_Encoding_CVPR_2021_paper.pdf)|CVPR|2021|R-101|Faster R-CNN |Fine-tuning|FSOD|32.9 44.0 46.8 52.9 59.7| 23.7 30.6 38.4 43.0 48.5 |22.6 33.4 39.5 47.3 54.0|[PyTorch](https://github.com/MegviiDetection/FSCE)|
|[Retentive R-CNN](https://openaccess.thecvf.com/content/CVPR2021/papers/Fan_Generalized_Few-Shot_Object_Detection_Without_Forgetting_CVPR_2021_paper.pdf)|CVPR|2021|R-101|R-CNN|Fine-tuning|gFSOD|71.3 72.3 72.1 74.0 74.6 | 66.8 68.4 70.2 70.7 71.5 |69.0 70.9 72.3 73.9 74.1|[PyTorch](https://github.com/Megvii-BaseDetection/GFSD)|
|[Halluc](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Hallucination_Improves_Few-Shot_Object_Detection_CVPR_2021_paper.pdf)|CVPR|2021|R-101|Faster R-CNN|Fine-tuning|FSOD|47.0 44.9 46.5 54.7 54.7 |26.3 31.8 37.4 37.4 41.2 |40.4 42.1 43.3 51.4 49.6|[-](https://github.com/pppplin/HallucFsDet)
|[TFA](https://arxiv.org/abs/2003.06957)| ICML|2020|R-101|Faster R-CNN|Fine-tuning|FSOD|39.8 36.1 44.7 55.7 56.0|23.5 26.9 34.1 35.1 39.1|30.8 34.8 42.8 49.5 49.8|[PyTorch](https://github.com/ucbdrive/few-shot-object-detection)|
|[FSDetView](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620188.pdf)|ECCV|2020|R-101|Faster R-CNN |meta-learning|FSOD|24.2 35.3 42.2 49.1 57.4|21.6 24.6 31.9 37.0 45.7|21.2 30.0 37.2 43.8 49.6|[PyTorch](http://imagine.enpc.fr/~xiaoy/FSDetView/)
|[MPSR](https://arxiv.org/pdf/2007.09384.pdf)| ECCV |2020|R-101|Faster R-CNN| Fine-tuning|FSOD|41.7 - 51.4 55.2 61.8| 24.4 - 39.2 39.9 47.8|35.6 - 42.3 48.0 49.7|[PyTorch](https://github.com/jiaxi-wu/MPSR)|
|[NP-RepMet](https://arxiv.org/pdf/2010.11714.pdf)|NeurIPS|2020|R-101| Faster R-CNN + DCN|Fine-tuning|FSOD|37.8 40.3 41.7 47.3 49.4|41.6 43.0 43.4 47.4 49.1| 33.3 38.0 39.8 41.5 44.8|[MXNet](https://github.com/yang-yk/NP-RepMet)|